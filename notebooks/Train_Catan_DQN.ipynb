{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catan DQN Training Notebook\n",
    "This notebook trains a Deep Q-Network (DQN) agent to play a simplified version of Settlers of Catan.\n",
    "\n",
    "### Setup\n",
    "1. Ensure you have the Catan-AI code in your python path.\n",
    "2. This notebook runs in headless mode (no pygame window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# CONFIG: Change this to the path where you uploaded the 'Catan-AI' folder in your Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/Catan-AI/code'\n",
    "\n",
    "# Copy code to local runtime for speed (reading from Drive is slow)\n",
    "if not os.path.exists('code'):\n",
    "    shutil.copytree(DRIVE_PATH, 'code')\n",
    "\n",
    "sys.path.append(os.path.abspath('code'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Add code directory to path\n",
    "if os.path.exists('code'):\n",
    "    sys.path.append(os.path.abspath('code'))\n",
    "else:\n",
    "    # Assuming we are inside 'notebooks'\n",
    "    sys.path.append(os.path.abspath('../code'))\n",
    "\n",
    "from catan_dqn_env import CatanEnv\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Store (s, a, r, s', done, next_mask)\n",
    "        self.memory = collections.deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done, next_mask):\n",
    "        self.memory.append((state, action, reward, next_state, done, next_mask))\n",
    "\n",
    "    def act(self, state, action_mask):\n",
    "        \"\"\"Epsilon-greedy, but never selects masked-out (invalid) actions.\"\"\"\n",
    "        valid_actions = np.flatnonzero(action_mask)\n",
    "        if valid_actions.size == 0:\n",
    "            return 0  # fallback to END_TURN\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return int(np.random.choice(valid_actions))\n",
    "\n",
    "        q = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        q = q.copy()\n",
    "        # Use dtype-safe negative infinity (avoids float16 overflow under mixed precision)\n",
    "        mask_value = np.finfo(q.dtype).min\n",
    "        q[~action_mask] = mask_value\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        next_masks = np.array([i[5] for i in minibatch])\n",
    "\n",
    "        target = self.model.predict(states, verbose=0)\n",
    "        target_next = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                qn = target_next[i].copy()\n",
    "                mask_value = np.finfo(qn.dtype).min\n",
    "                qn[~next_masks[i]] = mask_value\n",
    "                target[i][actions[i]] = rewards[i] + self.gamma * np.max(qn)\n",
    "\n",
    "        self.model.fit(states, target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CatanEnv()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Parameters from pseudocode\n",
    "EPISODES = 50\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_EVERY_N_ACTIONS = 4  # \"every n actions, sample k transitions and train\"\n",
    "UPDATE_TARGET_EVERY_M_EPISODES = 5  # \"every m episodes, copy training network to target\"\n",
    "\n",
    "scores = []\n",
    "total_actions = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # 1. Mask invalid actions\n",
    "        mask = env.action_mask()\n",
    "\n",
    "        # 2. Action (masked epsilon-greedy)\n",
    "        action = agent.act(state, mask)\n",
    "\n",
    "        # 3. Step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # 4. Next mask (for masked target max)\n",
    "        next_mask = env.action_mask() if not done else np.ones(action_size, dtype=bool)\n",
    "\n",
    "        # 5. Remember\n",
    "        agent.remember(state, action, reward, next_state, done, next_mask)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        total_actions += 1\n",
    "\n",
    "        # 6. Train every N actions\n",
    "        if total_actions % TRAIN_EVERY_N_ACTIONS == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "\n",
    "    # 7. Update Target Network every M episodes\n",
    "    if (e + 1) % UPDATE_TARGET_EVERY_M_EPISODES == 0:\n",
    "        agent.update_target_model()\n",
    "        print(f\"Target network updated at episode {e+1}\")\n",
    "\n",
    "    print(f\"episode: {e}/{EPISODES}, score: {score}, steps: {steps}, e: {agent.epsilon:.3}\")\n",
    "    scores.append(score)\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        agent.save(f\"catan-dqn-{e}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
