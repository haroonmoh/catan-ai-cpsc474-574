{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catan DQN Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "if not os.path.exists('catan-ai-cpsc474-574'):\n",
    "    !git clone https://github.com/haroonmoh/catan-ai-cpsc474-574\n",
    "\n",
    "if os.path.exists('catan-ai-cpsc474-574/code'):\n",
    "    sys.path.append(os.path.abspath('catan-ai-cpsc474-574/code'))\n",
    "elif os.path.exists('code'):\n",
    "    sys.path.append(os.path.abspath('code'))\n",
    "elif os.path.exists('../code'):\n",
    "    sys.path.append(os.path.abspath('../code'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from catan_dqn_env import CatanEnv\n",
    "\n",
    "# can use or not use. I am just using it for now for debugging\n",
    "np.random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = collections.deque(maxlen=100000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0 \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def record(self, state, action, reward, next_state, done, next_mask):\n",
    "        self.memory.append((state, action, reward, next_state, done, next_mask))\n",
    "\n",
    "    def act(self, state, action_mask):\n",
    "        valid_actions = np.flatnonzero(action_mask)\n",
    "        if valid_actions.size == 0:    # if no valid actions, return 0 which is END_TURN\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:    # explore with epsilon probability\n",
    "            return int(np.random.choice(valid_actions))\n",
    "\n",
    "        # get the q-values for all actions\n",
    "        q = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        q = q.copy()\n",
    "\n",
    "        # mask out invalid actions. again have to do this so it doesn't choose invalid actions\n",
    "        mask_value = np.finfo(q.dtype).min\n",
    "        q[~action_mask] = mask_value\n",
    "\n",
    "        # choose the action with the highest q-value\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # sample k transitions\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        next_masks = np.array([i[5] for i in minibatch])\n",
    "\n",
    "        # get the q-values for all actions\n",
    "        target = self.model.predict(states, verbose=0)\n",
    "        target_next = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                qn = target_next[i].copy()\n",
    "                mask_value = np.finfo(qn.dtype).min\n",
    "                qn[~next_masks[i]] = mask_value\n",
    "                # function from lecture r + gamma * max(q(s', a'))\n",
    "                target[i][actions[i]] = rewards[i] + self.gamma * np.max(qn)\n",
    "\n",
    "        # train q_train to match q_target\n",
    "        self.model.fit(states, target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CatanEnv()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "EPISODES = 1000\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_EVERY_N_ACTIONS = 4\n",
    "UPDATE_TARGET_EVERY_M_EPISODES = 10\n",
    "\n",
    "scores = []\n",
    "total_actions = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:   # while s is not terminal\n",
    "        mask = env.action_mask()        # masking out invalid actions\n",
    "        action = agent.act(state, mask)      # choose a with epsilon-greedy\n",
    "\n",
    "        # record (s, a, r, s')\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_mask = env.action_mask() if not done else np.ones(action_size, dtype=bool)\n",
    "        agent.record(state, action, reward, next_state, done, next_mask)\n",
    "\n",
    "        # update s <- s'\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        total_actions += 1\n",
    "        \n",
    "        # update every n actions pseudocode from lecture\n",
    "        if total_actions % TRAIN_EVERY_N_ACTIONS == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "\n",
    "    if (e + 1) % UPDATE_TARGET_EVERY_M_EPISODES == 0:\n",
    "        agent.update_target_model()\n",
    "        print(f\"Target network updated at episode {e+1}\")\n",
    "    \n",
    "    # Decay epsilon every 2 episodes (instead of every episode)\n",
    "    if (e + 1) % 2 == 0 and agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    print(f\"episode: {e}/{EPISODES}, score: {score:.2f}, steps: {steps}, e: {agent.epsilon:.3}\")\n",
    "    scores.append(score)\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        agent.save(f\"catan-dqn-{e}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
